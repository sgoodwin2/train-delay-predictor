{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcbccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Linear Regression Results:\n",
      "   MAE : 1.31 minutes\n",
      "   RMSE: 2.68 minutes\n",
      "\n",
      "📊 Random Forest Regressor Results:\n",
      "   MAE : 1.16 minutes\n",
      "   RMSE: 2.15 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv('dublin_connolly_clean_with_history.csv')\n",
    "\n",
    "prev_stations_num = 10\n",
    "prev_stations = [f'prev_station_{i}' for i in range(1, prev_stations_num+1)]\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['TrainOrigin', 'TrainDestination'] + prev_stations\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_final = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = df_final.drop(columns=['delay_minutes'])\n",
    "y = df_final['delay_minutes']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    print(f\"📊 {model_name} Results:\")\n",
    "    print(f\"   MAE : {mae:.2f} minutes\")\n",
    "    print(f\"   RMSE: {rmse:.2f} minutes\\n\")\n",
    "\n",
    "evaluate(y_test, y_pred_lr, \"Linear Regression\")\n",
    "evaluate(y_test, y_pred_rf, \"Random Forest Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3d7c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stang/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 52.7274 - val_loss: 8.1231\n",
      "Epoch 2/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.6189 - val_loss: 6.6613\n",
      "Epoch 3/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 19.3167 - val_loss: 6.6667\n",
      "Epoch 4/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.1450 - val_loss: 6.2563\n",
      "Epoch 5/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 12.8019 - val_loss: 5.7192\n",
      "Epoch 6/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 31.5439 - val_loss: 5.5445\n",
      "Epoch 7/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.5268 - val_loss: 5.0341\n",
      "Epoch 8/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.3440 - val_loss: 4.7758\n",
      "Epoch 9/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.0494 - val_loss: 4.4328\n",
      "Epoch 10/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 28.4531 - val_loss: 4.5699\n",
      "Epoch 11/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6307 - val_loss: 4.4725\n",
      "Epoch 12/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.3283 - val_loss: 4.7722\n",
      "Epoch 13/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4795 - val_loss: 4.6245\n",
      "Epoch 14/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 12.8638 - val_loss: 5.0354\n",
      "Epoch 15/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.9380 - val_loss: 4.9724\n",
      "Epoch 16/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6350 - val_loss: 5.5383\n",
      "Epoch 17/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.7416 - val_loss: 5.2271\n",
      "Epoch 18/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2788 - val_loss: 6.0770\n",
      "Epoch 19/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.4566 - val_loss: 6.1439\n",
      "Epoch 20/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.5174 - val_loss: 6.5381\n",
      "Epoch 21/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.0995 - val_loss: 6.4935\n",
      "Epoch 22/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.6931 - val_loss: 6.7557\n",
      "Epoch 23/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1486 - val_loss: 7.4798\n",
      "Epoch 24/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 12.0313 - val_loss: 8.4661\n",
      "Epoch 25/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9229 - val_loss: 7.9014\n",
      "Epoch 26/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4534 - val_loss: 8.4509\n",
      "Epoch 27/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7742 - val_loss: 8.8192\n",
      "Epoch 28/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7125 - val_loss: 9.1992\n",
      "Epoch 29/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6865 - val_loss: 10.5167\n",
      "Epoch 30/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4934 - val_loss: 9.8679\n",
      "Epoch 31/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6984 - val_loss: 10.4002\n",
      "Epoch 32/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2417 - val_loss: 10.7183\n",
      "Epoch 33/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5401 - val_loss: 9.8301\n",
      "Epoch 34/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.3338 - val_loss: 9.5696\n",
      "Epoch 35/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0220 - val_loss: 9.5891\n",
      "Epoch 36/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.8625 - val_loss: 9.6628\n",
      "Epoch 37/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.4322 - val_loss: 9.3756\n",
      "Epoch 38/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1409 - val_loss: 9.6907\n",
      "Epoch 39/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3421 - val_loss: 7.9305\n",
      "Epoch 40/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3139 - val_loss: 5.9100\n",
      "Epoch 41/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.0509 - val_loss: 5.7736\n",
      "Epoch 42/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4209 - val_loss: 5.7203\n",
      "Epoch 43/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3775 - val_loss: 6.4633\n",
      "Epoch 44/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2994 - val_loss: 5.6201\n",
      "Epoch 45/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2567 - val_loss: 5.5727\n",
      "Epoch 46/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8191 - val_loss: 5.3941\n",
      "Epoch 47/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0005 - val_loss: 5.5694\n",
      "Epoch 48/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2622 - val_loss: 5.3135\n",
      "Epoch 49/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2220 - val_loss: 5.3417\n",
      "Epoch 50/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2676 - val_loss: 5.4343\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "📊 Recurrent Neural Network (RNN) Results:\n",
      "   MAE : 1.40 minutes\n",
      "   RMSE: 5.87 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('dublin_connolly_clean_with_history.csv')\n",
    "\n",
    "prev_stations_num = 10\n",
    "prev_stations = [f'prev_station_{i}' for i in range(1, prev_stations_num+1)]\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['TrainOrigin', 'TrainDestination'] + prev_stations\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_final = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = df_final.drop(columns=['delay_minutes'])\n",
    "y = df_final['delay_minutes']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input for RNN: [samples, time steps, features]\n",
    "# Here we assume each sample is one timestep (you could adapt for sequences if needed)\n",
    "X_train_rnn = np.expand_dims(X_train, axis=1)  # Shape: (samples, 1, features)\n",
    "X_test_rnn = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Build RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, activation='tanh', input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_rnn, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rnn = model.predict(X_test_rnn).flatten()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    print(f\"📊 {model_name} Results:\")\n",
    "    print(f\"   MAE : {mae:.2f} minutes\")\n",
    "    print(f\"   RMSE: {rmse:.2f} minutes\\n\")\n",
    "\n",
    "# Evaluate RNN\n",
    "evaluate(y_test, y_pred_rnn, \"Recurrent Neural Network (RNN)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2986ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stang/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 23.1736 - val_loss: 7.0057\n",
      "Epoch 2/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.7590 - val_loss: 6.4092\n",
      "Epoch 3/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.7703 - val_loss: 6.3203\n",
      "Epoch 4/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 23.1293 - val_loss: 6.0988\n",
      "Epoch 5/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 16.5447 - val_loss: 6.2140\n",
      "Epoch 6/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.5547 - val_loss: 5.9176\n",
      "Epoch 7/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.7983 - val_loss: 5.9144\n",
      "Epoch 8/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 27.9917 - val_loss: 5.9328\n",
      "Epoch 9/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 26.1562 - val_loss: 6.1184\n",
      "Epoch 10/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17.5582 - val_loss: 5.8020\n",
      "Epoch 11/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.1200 - val_loss: 5.8052\n",
      "Epoch 12/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.8183 - val_loss: 5.9245\n",
      "Epoch 13/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.3751 - val_loss: 5.8004\n",
      "Epoch 14/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.9422 - val_loss: 5.7358\n",
      "Epoch 15/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9713 - val_loss: 5.6793\n",
      "Epoch 16/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6150 - val_loss: 5.8265\n",
      "Epoch 17/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.8301 - val_loss: 5.7949\n",
      "Epoch 18/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.5292 - val_loss: 5.6673\n",
      "Epoch 19/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7025 - val_loss: 6.1464\n",
      "Epoch 20/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.6398 - val_loss: 5.7792\n",
      "Epoch 21/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0812 - val_loss: 6.0491\n",
      "Epoch 22/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17.5904 - val_loss: 5.9236\n",
      "Epoch 23/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.3091 - val_loss: 5.7920\n",
      "Epoch 24/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.3831 - val_loss: 5.7474\n",
      "Epoch 25/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0481 - val_loss: 5.7254\n",
      "Epoch 26/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6412 - val_loss: 5.9691\n",
      "Epoch 27/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6567 - val_loss: 5.7360\n",
      "Epoch 28/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8505 - val_loss: 5.8590\n",
      "Epoch 29/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.1507 - val_loss: 5.7975\n",
      "Epoch 30/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.0296 - val_loss: 5.7391\n",
      "Epoch 31/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1927 - val_loss: 6.0578\n",
      "Epoch 32/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6411 - val_loss: 5.9690\n",
      "Epoch 33/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7512 - val_loss: 5.8245\n",
      "Epoch 34/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0395 - val_loss: 5.7719\n",
      "Epoch 35/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0308 - val_loss: 5.6622\n",
      "Epoch 36/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0414 - val_loss: 5.6755\n",
      "Epoch 37/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8586 - val_loss: 5.7294\n",
      "Epoch 38/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9275 - val_loss: 5.7683\n",
      "Epoch 39/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1377 - val_loss: 5.6761\n",
      "Epoch 40/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0169 - val_loss: 5.6664\n",
      "Epoch 41/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9546 - val_loss: 5.6956\n",
      "Epoch 42/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6486 - val_loss: 5.6983\n",
      "Epoch 43/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.2528 - val_loss: 5.2733\n",
      "Epoch 44/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8392 - val_loss: 5.2622\n",
      "Epoch 45/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9500 - val_loss: 5.7445\n",
      "Epoch 46/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.8533 - val_loss: 5.6829\n",
      "Epoch 47/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3725 - val_loss: 5.2473\n",
      "Epoch 48/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9814 - val_loss: 5.7618\n",
      "Epoch 49/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 10.1618 - val_loss: 5.0491\n",
      "Epoch 50/50\n",
      "\u001b[1m335/335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8734 - val_loss: 5.2496\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "📊 Long Short-Term Memory (LSTM) Results:\n",
      "   MAE : 1.24 minutes\n",
      "   RMSE: 2.38 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('dublin_connolly_clean_with_history.csv')\n",
    "\n",
    "prev_stations_num = 10\n",
    "prev_stations = [f'prev_station_{i}' for i in range(1, prev_stations_num+1)]\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = ['TrainOrigin', 'TrainDestination'] + prev_stations\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_final = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = df_final.drop(columns=['delay_minutes'])\n",
    "y = df_final['delay_minutes']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input for LSTM: [samples, time steps, features]\n",
    "X_train_lstm = np.expand_dims(X_train, axis=1)  # One time step per sample\n",
    "X_test_lstm = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='tanh', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lstm = model.predict(X_test_lstm).flatten()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    print(f\"📊 {model_name} Results:\")\n",
    "    print(f\"   MAE : {mae:.2f} minutes\")\n",
    "    print(f\"   RMSE: {rmse:.2f} minutes\\n\")\n",
    "\n",
    "# Evaluate LSTM\n",
    "evaluate(y_test, y_pred_lstm, \"Long Short-Term Memory (LSTM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2fd5c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrainOrigin</th>\n",
       "      <th>TrainDestination</th>\n",
       "      <th>scheduled_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>prev_station_1</th>\n",
       "      <th>prev_delay_1</th>\n",
       "      <th>prev_station_2</th>\n",
       "      <th>prev_delay_2</th>\n",
       "      <th>prev_station_3</th>\n",
       "      <th>prev_delay_3</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_delay_6</th>\n",
       "      <th>prev_station_7</th>\n",
       "      <th>prev_delay_7</th>\n",
       "      <th>prev_station_8</th>\n",
       "      <th>prev_delay_8</th>\n",
       "      <th>prev_station_9</th>\n",
       "      <th>prev_delay_9</th>\n",
       "      <th>prev_station_10</th>\n",
       "      <th>prev_delay_10</th>\n",
       "      <th>delay_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dublin Connolly</td>\n",
       "      <td>Belfast</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dublin Connolly</td>\n",
       "      <td>Belfast</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dublin Connolly</td>\n",
       "      <td>Belfast</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dublin Connolly</td>\n",
       "      <td>Belfast</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dublin Connolly</td>\n",
       "      <td>Portadown</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14878</th>\n",
       "      <td>Greystones</td>\n",
       "      <td>Howth</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Tara Street</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Dublin Pearse</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Grand Canal Dock</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Booterstown</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Blackrock</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Seapoint</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Salthill and Monkstown</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14879</th>\n",
       "      <td>Bray</td>\n",
       "      <td>Howth</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Tara Street</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>Dublin Pearse</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Grand Canal Dock</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Booterstown</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Blackrock</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Seapoint</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Salthill and Monkstown</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14880</th>\n",
       "      <td>Bray</td>\n",
       "      <td>Howth</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>Tara Street</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Dublin Pearse</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Grand Canal Dock</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Booterstown</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Blackrock</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Seapoint</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Salthill and Monkstown</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14881</th>\n",
       "      <td>Bray</td>\n",
       "      <td>Howth</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Tara Street</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Dublin Pearse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Grand Canal Dock</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Booterstown</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Blackrock</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Seapoint</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Salthill and Monkstown</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14882</th>\n",
       "      <td>Bray</td>\n",
       "      <td>Howth</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Tara Street</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Dublin Pearse</td>\n",
       "      <td>2.6</td>\n",
       "      <td>Grand Canal Dock</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Booterstown</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Blackrock</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Seapoint</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Salthill and Monkstown</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14883 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TrainOrigin TrainDestination  scheduled_hour  day_of_week  \\\n",
       "0      Dublin Connolly          Belfast               8            1   \n",
       "1      Dublin Connolly          Belfast               8            5   \n",
       "2      Dublin Connolly          Belfast               8            5   \n",
       "3      Dublin Connolly          Belfast               8            4   \n",
       "4      Dublin Connolly        Portadown               8            1   \n",
       "...                ...              ...             ...          ...   \n",
       "14878       Greystones            Howth               9            1   \n",
       "14879             Bray            Howth               9            2   \n",
       "14880             Bray            Howth               9            4   \n",
       "14881             Bray            Howth               9            1   \n",
       "14882             Bray            Howth               9            3   \n",
       "\n",
       "      prev_station_1  prev_delay_1 prev_station_2  prev_delay_2  \\\n",
       "0                NaN           0.0            NaN           0.0   \n",
       "1                NaN           0.0            NaN           0.0   \n",
       "2                NaN           0.0            NaN           0.0   \n",
       "3                NaN           0.0            NaN           0.0   \n",
       "4                NaN           0.0            NaN           0.0   \n",
       "...              ...           ...            ...           ...   \n",
       "14878    Tara Street           1.8  Dublin Pearse           1.9   \n",
       "14879    Tara Street          -0.4  Dublin Pearse          -0.1   \n",
       "14880    Tara Street           0.1  Dublin Pearse           0.5   \n",
       "14881    Tara Street          -0.1  Dublin Pearse           0.0   \n",
       "14882    Tara Street           2.3  Dublin Pearse           2.6   \n",
       "\n",
       "         prev_station_3  prev_delay_3  ... prev_delay_6  prev_station_7  \\\n",
       "0                   NaN           0.0  ...          0.0             NaN   \n",
       "1                   NaN           0.0  ...          0.0             NaN   \n",
       "2                   NaN           0.0  ...          0.0             NaN   \n",
       "3                   NaN           0.0  ...          0.0             NaN   \n",
       "4                   NaN           0.0  ...          0.0             NaN   \n",
       "...                 ...           ...  ...          ...             ...   \n",
       "14878  Grand Canal Dock           2.4  ...          2.8     Booterstown   \n",
       "14879  Grand Canal Dock           0.2  ...          0.5     Booterstown   \n",
       "14880  Grand Canal Dock           0.2  ...          0.8     Booterstown   \n",
       "14881  Grand Canal Dock          -0.1  ...          0.6     Booterstown   \n",
       "14882  Grand Canal Dock           2.8  ...          1.6     Booterstown   \n",
       "\n",
       "      prev_delay_7  prev_station_8 prev_delay_8  prev_station_9 prev_delay_9  \\\n",
       "0              0.0             NaN          0.0             NaN          0.0   \n",
       "1              0.0             NaN          0.0             NaN          0.0   \n",
       "2              0.0             NaN          0.0             NaN          0.0   \n",
       "3              0.0             NaN          0.0             NaN          0.0   \n",
       "4              0.0             NaN          0.0             NaN          0.0   \n",
       "...            ...             ...          ...             ...          ...   \n",
       "14878          2.8       Blackrock          2.2        Seapoint          2.0   \n",
       "14879          0.5       Blackrock          0.3        Seapoint          0.3   \n",
       "14880          1.5       Blackrock          0.8        Seapoint          0.9   \n",
       "14881          0.9       Blackrock          0.4        Seapoint          0.5   \n",
       "14882          1.9       Blackrock          1.5        Seapoint          1.3   \n",
       "\n",
       "              prev_station_10 prev_delay_10  delay_minutes  \n",
       "0                         NaN           0.0            5.2  \n",
       "1                         NaN           0.0            1.1  \n",
       "2                         NaN           0.0            0.3  \n",
       "3                         NaN           0.0            4.5  \n",
       "4                         NaN           0.0            0.0  \n",
       "...                       ...           ...            ...  \n",
       "14878  Salthill and Monkstown           2.0            2.6  \n",
       "14879  Salthill and Monkstown           0.4            2.1  \n",
       "14880  Salthill and Monkstown           0.9            0.5  \n",
       "14881  Salthill and Monkstown           0.6            0.7  \n",
       "14882  Salthill and Monkstown           1.3            2.2  \n",
       "\n",
       "[14883 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
